### 实验原理

本章会详细论证我们的方法的统计原理。在介绍了我们主要使用的变分自编码器(VAE)和生成对抗网络(GAN)之后，我们将推导出本文的方法的统计基础，为下一章介绍我们的方法的具体实现做好铺垫。

生成模型旨在学习一个分布X~P(X)，其中X可能是来自一个高维空间的数据点，例如我们实验的图像数据来自128\*128\*3维的分布。根据实际应用的需要，生成模型更关心的是从P(X)中采样，而不是数值的学习到P（X），后者并不一定对前者有帮助。变分自编码器和生成对抗网络是成功的生成模型的两个例子，他们成功的利用了神经网络来拟合P(X)，并且对数据分布并没有很强的先验假设，因而成为广泛应用的生成模型。

##### 变分自编码器

变分自编码器主要可以用来无监督的学习复杂的数据分布。通过将直接采样X分解为先采样一个隐变量z，再通过学习到的映射将z映射到X的空间中，VAE将一个复杂的采样任务分解成一个从隐空间采样和一个学习映射的任务。这样做的优点有，1）因为z从属的空间往往是简单的空间，例如标准正态分布，隐空间中的采样任务很容易实现；2）映射往往用神经网络来实现，这样容易通过随机梯度下降的方法优化，并且计算量也不会十分巨大。公式化的话，VAE期望优化训练集中的每一个数据点的概率值，P(X) = \int P(X | z; \theta) P(z) dz，其中由z生成X的分布是由\theta参数化的神经网络，为方便后续推导，我们假定 P(X | z; \theta) = N(X | f(z; theta), \sigma^2 \* I)。如果直接对上式采样，将意味着很大的计算负担，并且对于大多数的z，P(X|z)都接近于0，很多样本是近似无意义的。因此，变分自编码器需要另一个函数Q(z|X)来拟合真实的P（z|X）分布，这也正是名字中变分一词的来历。其中Q分布通常也假定为正态分布，由另一个模型估计出来。这样，通过观察下式

$KL(Q(z|X) ||P(z|X)) = E_{z \sim Q}[\log Q(z|X) - \log P(z | X)]$

$KL(Q(z|X) ||P(z|X)) = E_{z \sim Q}[\log Q(z|X) - \log P(X | z) - \log P(z)] + \log P(X)$

$\log P(X) - KL(Q(z|X) | P(z|X)) = E_{z \sim Q}[\log P(X|z)] - KL(Q(z) | P(z))$

上式左边正是我们想要优化的：两项分别代表最大化训练集的概率，和最小化变分带来的误差。而上式右边两项又可以通过神经网络优化：两项分别代表重构图片的概率，在高斯假定的情况下近似为L2误差，和两个正态分布之间的KL散度。



插一张图：VAE tutorial fig1，重参数化的图



从表示学习(representation learning)的角度看，VAE学习到了高维空间图像的一个低维表示。在表示学习中，一个重要的任务是学习到一个分离(disentangle)的表示，这种表示既存储了高维空间向量的全部信息，本身又有很强的可解释性。例如对于手写字符任务，一种可行的分离表示可以是n维隐变量的第一个维度表示字符的类别，第二/三个表示字符的斜度/粗细，每一个维度表征数据集中变化的一个因子（factor of variation）。这种表示意味着我们可以通过调整一个维度来调整生成图像的某一个特征，对风格迁移任务有很大的帮助。在传统的VAE基础之上，研究者提出了不同的VAE变种来提升VAE编码的分离能力，如. Understanding disentangling in beta-vae Learning disentangled representations with wasserstein autoencoders. 他们可以看作在传统的VAE基础上加入

$\int Q(z|X) P(X) dX$

，用不同的方式实现这个后验分布，优化模型的分离能力。



上述的分离学习方法专注于在隐变量的每一个维度都分离的控制不同的变化因子。在细粒度的分离学习之外，我们也可以将VAE的隐变量分成几组，每组控制到不同的特征。A Variational U-Net for Conditional Appearance and Shape Generation文认为图像是由形状(shape)和外表(appearance)共同控制生成，文中提出了一个条件的U-Net，用形状和RGB图片生成新的图片。DISENTANGLING CONTENT AND STYLE VIA UNSUPERVISED GEOMETRY DISTILLATION文用无监督的方法分离图像中的结构和风格信息。本文同样以VAE为理论基础，专注于分离图片中几何信息和风格信息。我们在理论推导方面收到了上两文的启发，但选取了深度图表示结构信息，而不是隐变量或关键点。



##### 生成对抗网络



生成对抗网络（GAN）是另一种常用的生成模型，与VAE不同，它并不直接估计数据的概率分布，而是间接的近似$P(X)$。GAN的模型分成两个部分：生成器和判别器，前者能采样出和训练集数据同分布的样本，而判别器能够判别一个给定样本是否来自于训练集的分布，二者通常是以神经网络参数化的模型。GAN的训练通过上述两方的博弈，最终得到一个纳什均衡。和VAE相比，GAN的优化目标并没有用到变分下界，并且神经网络的优秀拟合能力使得GAN理论上是能够拟合任何分布的。另外，由于GAN的优化目标是判断训练集的分布和生成器生成的分布是否拟合，对每一个样本从整体上判断生成质量，经验上它所生成的样本往往更具真实性。而VAE由于损失函数中的重构损失往往由L1/2损失函数估计，这种像素级别的损失函数偏向于生成一个平均图像，导致生成图像往往边界不清晰，图像模糊。

公式化论述的话，我们遵从GAN原文的记法，训练集$x \sim p_{data}(x)$，生成器和判别器分别记为$G, D$，是两个神经网络，前者接受一个随机变量$z$作为输入，它通常来自一个标准正态分布，力图输出符合分布$p_{data}$的样本，后者类似一个分类器，接受一个样本，输出$[0, 1]$之间的数值，力图对训练集中样本输出1，而对生成器生成的样本输出0。按前文的说明，GAN要优化的是下式

$\min_{G} \max_{D} V(D, G) = E_{x \sim p_{data}(x)}[\log D(x)] + E_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]$

GAN原文中证明了上式事实上等价于优化生成器的分布和训练集的Jensen-Shannon散度，亦即对称的KL散度，并且也在理论上论述了最优解的存在性。



在材质迁移的任务中，由于融合了两张图像的材质和内容的图像不可得，因而无法使用L1/2损失函数，在这种情况下，往往使用GAN来判断生成图片的质量。而仅仅判断要求生成图片属于训练集的分布是不足以要求生成图片融合了不同的风格和内容的。一种可行的改进是使用判别器接受成对的图片判别是否拥有同一材质或内容，这要求合理的利用训练数据构造真和假的数据对。图像生成任务中，另一种对生成对抗网络的改进是使用多尺度的判别器，这种方法利用多个判别器分别接受原尺度和按不同比例缩小的图片，使得生成图片在各个尺度上都能拟合训练集的特征。利用progan的角度来看，我们也可以认为多尺度的判别器事实上为生成器提供了更多的信息，使得它能够从小尺度(也就是全局)的特征开始学习，最后去拟合大尺度（细节）的特征。本文的方法使用了上述两种对生成对抗网络的改进，使生成图像更加真实。



#### 本文实验原理

我们首先正式的阐述本文研究的问题，并且明确后文中会用到的记号。三维模型的最优视图生成任务即针对一个给定的三维模型$m \in M$，我们的模型能够生成一张图片$i \in I = R^{m \times n \times 3}$，在后文中除非特殊说明$m = n = 128$。我们将这个任务拆解为给定一个视角$v$，将三维模型投影为深度图和给定一个材质$s$，将深度图渲染为RGB图片的问题，如下式。

$i_{d} = proj(m, v) \in R^{m \times n}, vp = [az, el, dist]$

$i_{out} = f(i_{d}, s)$

前者可以确定性的通过Blender渲染，关于视角选择的问题会在后文中阐述。而后者我们应用了VAE和GAN来指导生成，在这里我们假定每一张RGB图片$i \in I = R^{m \times n \times 3}$都由内容、材质和视角三个独立的变量指导生成，而每一张深度图$i_{d} \in R^{m \times n}$只由内容和视角两个互相独立的变量指导生成，其中内容和材质是需要推断的隐变量，而视角在训练时给出，是已知的变量。因此为从深度图生成RGB图片需要额外加入一个材质信息指导生成，这个信息可以从真实图片中提取出来，也可以是隐空间采样的随机变量。概率化表示的话：

$i_{out} \sim P(i | c, s, v), i_{d} \sim P(i | c, v)$

类似前文对VAE的推导，我们训练过程中优化训练集的概率

$ \log p(i) = \log p(c) + \log p(i | c) - \log p(c | i)$

$\geq \log p(c) + \log  \int p(i, s | c) ds$

$\geq \log p(c) + E_{q} \log \frac{p(i, s | c)}{q(s | i, c)}$

$= \log p(c) + E_{q} \log \frac{p(i | c, s)p(s)}{q(s | i)}$

$ = \log p(c) + E_{q} \log p(i | c, s) + KL(q(s) || p(s))$

$ \log p(i | v)= \log p(c) + E_{q} \log p(i | c, s, v) + KL(q(s) || p(s))$

其中第一项代表我们推导出的内容隐变量的概率，它越表征内容信息，这个概率值应越大，在实验中我们通过网络重构出的深度图和真实深度图之间的L1损失衡量，注意与猫猫文提出的在关键点上的弱监督不同，我们在这里加的是一个很强的限制条件。第二项即为重构出的图片的真实性，我们遵从VAE的惯例，用L1损失实现。第三项为推导出的材质隐变量的KL散度损失，用两个正态分布之间的KL散度实现。

上述损失函数只描述了重构图片时的损失，单独实现这三个损失函数只保证了我们模型重构图片和采样的精度，但并不能保证我们假定的隐变量之间互相独立。为保证视角和其他隐变量相互独立，我们在训练过程中给出所有图片的视角，并且设计了新视角生成的实验：从输入图片中提取内容和材质隐变量，在另一个视角下生成输出图片

$i_{out} \sim P(i | c, s, v^{1}), i_{d} \sim P(i | c, v^{2})$

在这个实验中，视角$v^{1}, v^{2}​$均在训练时给出，损失函数的推导同上，损失函数的实现也完全相同。为保证内容和材质变量向独立，我们设计了交换材质迁移的实验

$i_{out} \sim P(i | c, s, v^{1}), c = P(c | i^{c}), s = P(s | i^{s})$



（插几个图）

