% Copyright (c) 2019 Bochen Tan
% Public domain.
%本模板的宗旨是尽量绿色，不需要附加安装任何东西。
%按照教务部下发的WORD说明文档格式，下简称“说明”
%没有封面和评阅表，这两部分请直接在Cover&ReviewTable.doc中写再输出pdf拼到一起
%doc小改动：封面校徽和文字替换为了高清版本，“题目：”和中文题目对齐，中英文题目分在了表的两行
%doc小改动：插入了两个白页，使得连续打印的时候封面和表格都在奇数页
%正文部分改动：在每一页下方中央加了页码，因为说明中页眉不分奇偶页，所以页码就都在中央吧
%不含自动的参考文献，因为说明中参考文献格式不典型，请手动输入或自行写程序
%在Windows或Linux下渲染出字体更接近说明，Mac OS上字体不太一样
%有警告\headheight is too small，fancyhdr的上距离有点小，似乎问题不大

\documentclass[UTF8,openany,AutoFakeBold,AutoFakeSlant,cs4size]{ctexbook}
%openany 使一章可以从偶数页开始，因为说明中每一章并没有只能从奇数页开始，虽然这是常理
%AutoFakeBold 和 AutoFakeSlant 因为 CJK 里没有真正的加粗和倾斜，如果额外字体则效果更好
%cs4size 因为要求主题是小四号字

\usepackage[a4paper,left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm]{geometry}
%office中正常页边距



\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{fancyhdr}



\usepackage{cite}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}} %引用在右上角



\usepackage{multirow,booktabs,makecell}
\usepackage{graphicx}
\usepackage[font=small,labelsep=space]{caption} %五号，宋体/Time new roman
\renewcommand{\thetable}{\arabic{table}} %表格和图片编号不分章节，直接1，2，3 ...
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\theequation}{\arabic{chapter}.\arabic{equation}} %公式标签 章.公式（均为阿拉伯数字）



\usepackage{tocloft} %自定义目录，说明中没有明确规定，和WORD自动生成目录格式一致

%“全文目录”四个字的格式
\renewcommand\cftbeforetoctitleskip{0pt}
\renewcommand\cftaftertoctitleskip{0pt}
\renewcommand\cfttoctitlefont{\bfseries\heiti\zihao{2}}

\renewcommand\cftchapfont{\heiti\normalsize} %黑体小四
\renewcommand\cftchapdotsep{\cftdotsep} %有点连到页码，点间距不确定，待改
\renewcommand\cftchappagefont{\songti\normalsize} %宋体小四页码
\renewcommand\cftbeforechapskip{0pt}

%1. 第一级 五号宋体，缩进两个字符，页码一致
\renewcommand\cftsecfont{\songti\small}
\renewcommand\cftsecpagefont{\songti\small}
\renewcommand\cftsecaftersnum{.} %一级目录号后加点
\renewcommand\cftsecindent{2em}
\renewcommand\cftbeforesecskip{0pt}

%1.1 第二级 五号宋体，缩进四个字符，页码一致
\renewcommand\cftsubsecfont{\songti\small}
\renewcommand\cftsubsecpagefont{\songti\small}
\renewcommand\cftsubsecindent{4em}
\renewcommand\cftbeforesubsecskip{0pt}

%1.1.1 第二级 五号宋体，缩进四个字符，页码一致
\renewcommand\cftsubsubsecfont{\songti\small}
\renewcommand\cftsubsubsecpagefont{\songti\small}
\renewcommand\cftsubsubsecindent{4em}
\renewcommand\cftbeforesubsubsecskip{0pt}



\usepackage{titlesec}%自定义章节标题
\CTEXsetup[format={\bfseries\center\heiti\zihao{2}},beforeskip=0pt]{chapter}
%第一章  绪论（二号、黑体） beforeskip为上方垂直距离看起来还比说明偏大，待改

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
%使目录中有三级标题，即subsubsection

\renewcommand\thesection{\arabic{section}} % 使得不显示章名，只显示节名
\titleformat{\section}
{\raggedright\zihao{3}\bfseries\songti}
{\thesection.\quad}
{0pt}
{}%1. 第一级（三号、宋体/Time new roman、加粗）

\titleformat{\subsection}
{\raggedright\bfseries\zihao{4}\songti}
{\thesubsection\quad}
{0pt}
{}%1.1 第二级（四号，宋体/Time new roman，加粗）

\titleformat{\subsubsection}
{\raggedright\bfseries\zihao{-4}\songti}
{\thesubsubsection\quad}
{0pt}
{}%1.1.1 第三级（小四，宋体/Time new roman，加粗）




% 封面依赖的宏包
\input{CoverHead}
% 评阅表依赖的宏包
\input{ReviewTableHead}



\title{}
\author{}
\date{}
\begin{document}

% 封面中需要修改的内容直接在此处更改即可
\newcommand{\chineseTitle}{三维模型最优二维视图生成方法研究}
\newcommand{\englishTitle}{Synthesizing best 2D views of 3D models}
\newcommand{\name}{黄道吉}
\newcommand{\studentID}{1600017857}
\newcommand{\school}{元培学院}
\newcommand{\major}{计算机科学与技术}
\newcommand{\advisor}{连宙辉}
% 插入封面
\input{cover}
\clearpage




%版权声明后空白一页，使得摘要从奇数页开始。
\quad
\setcounter{page}{0}
% 本页不计页码
\thispagestyle{empty}
% 本页无页眉和页脚
\clearpage



\pagestyle{fancy}
\normalsize
\linespread{1.5}\selectfont
%小四号，宋体/Time new roman，1.5倍行距
\chapter*{摘要}

生成三维模型的最优视图任务要求给定一个三维模型，我们能够选取出合适的视角，并且在这个视角下渲染出具有真实性的图片。随着三维建模方法的不断完善，三维模型的使用在近几年正变的越来越广泛。大规模三维模型数据库的出现更加便捷了有关三维模型的研究的进展。而三维模型不同于二维图片的特性使得它需要经过渲染才能显示在屏幕上。生成符合人类感知的三维模型的最优视图，将大大方便三维模型库的检索。近年来神经网络在二维视图生成工作的进展飞速，这也使得借助神经网络来生成三维模型的最优视图成为可能，尤其是生成对抗网络和变分自编码器在条件生成和非条件生成领域取得了很好的效果。本文回顾了以往在这个方向上研究者的工作，包括最优视角选择和新视角生成的算法，并提出了新的生成三维模型视图的方法。我们的方法首先能够根据一张导引图片提供的材质信息来渲染给定的三维模型在某一个视角下的视图，也能够通过在隐空间中采样来无条件的生成多样的三维模型的视图。定性和定量的实验结果表明，我们的新算法能够产生更加准确、更具真实性的三维模型图像，也在选择视角方面相对其他算法有自己的优势。最后，我们总结了本文的成果并展望了未来的工作。

\bigskip
\noindent{\bfseries\songti 关键词: 最优视角选择\ 新视角生成\ 材质迁移}



\addcontentsline{toc}{chapter}{摘要} %手动加入目录
\fancypagestyle{plain} %因为latex默认每章第一页是plain所以需要重置一下plain和说明统一
{
	\fancyhf{} %清空

	\fancyhead[RE,RO]{摘要}
	%偶数页右页眉，奇数页右页眉均为“摘要”，及章名\leftmark

	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	%偶数页左页眉，奇数页左页眉均为“北京大学本科生毕业论文”

	\fancyfoot[CO,CE]{~\thepage~}
	%偶数页和奇数页中页脚为页码，从对称考虑，因为每页在说明中都是一样的，不分奇偶

	\renewcommand{\headrulewidth}{0.7pt} %页眉线宽度，可调，不太清楚说明中是多少，待改

	\renewcommand{\footrulewidth}{0pt} %页脚线宽度为0，既没有
}

%默认的风格是fancy，设置于下，用于每章非第一页
\fancyhf{}
\fancyhead[RE,RO]{摘要}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}
\clearpage






\small
\linespread{1.5}\selectfont
%5号，Time new roman，1.5倍行距
\chapter*{\bfseries Abstract}

Synthesizing the best view of 3D models aims to choose to best viewpoint and rendering a realistic image given a 3D model. With the development of 3D modeling, the use of 3D models has been more and more prevalent. The emergent of 3D databases further advocates the development of 3D model related research. Unlike 2D images, 3D models have to be rendered to be displayed on a screen. Synthesizing the best view of 3D models in line with human perception will contribute to the retrieval of 3D databases. Thanks to the rapid development of neural networks' application on synthesizing 2D images, it has been possible in recent years to synthesize the best view of 3D models using neural networks, especially GAN and VAE, which yields great results in conditional and unconditional image synthesis. This paper reviews past research on this field, including best view synthesis and novel view synthesis, and propose a novel method to synthesize views of 3D models. First, our model can render an image of a given 3D model under a specific viewpoint while being consistent in style to a reference image. Second, our method can render various views of 3D models by sampling in latent space. Our new method is proved to be able to yield more accurate and realistic images through qualitative and quantitative experiments and outperforms previous best view selection methods. Finally, we summarize our contribution and proposed future work on this topic.

\bigskip
\noindent
{\bfseries Key Words: best view selection\ \ novel view synthesis\ \ style transfer}



\addcontentsline{toc}{chapter}{\bfseries Abstract} %Abstract加粗
\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{Abstract}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{Abstract}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}
\clearpage





\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{全文目录}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{全文目录}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\contentsname}{\centerline{全文目录}}
\tableofcontents
\addcontentsline{toc}{chapter}{全文目录}
\clearpage





\normalsize
\linespread{1.5}\selectfont
%正文，小四号，中文宋体，英文Time new roman，1.5倍行距
\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{\leftmark}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{\leftmark}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}



\chapter{引言}

\section{研究背景}

三维模型是图形学和计算机视觉方向的研究重点。近年来，三维模型的应用变得越来越广泛，从游戏界和工业界的 CAD 模型，到前沿领域的自动驾驶，使用三维模型正大大便利着业界。RGB-D 传感器的应用
也使得产生三维模型更加容易。在学术界，三维模型也有着广泛的应用：三维模型的分割(\cite{Chen2009ABF, Kundu2014JointSS})、重建(\cite{Choy20163DR2N2AU, Mandikal20183DLMNetLE})，以及利用三维模型强化对图片的理解(\cite{Choy2015EnrichingOD})。这些因素都催生了大规模三维模型库的产生和广泛使用(如Shapenet \cite{Chang2015ShapeNetAI}, Pascal3D+ \cite{Xiang2014BeyondPA}, ModelNet\cite{Wu20143DSA})。

在如此多的精力投入利用数据集解决问题的同时，相对少的精力投入到利用数据驱动的方法方便数据集的可视化和检索上。不同于二维图片便于观看、容易生成缩略图，三维模型在不同视角下会有不同的姿态，并且需要材质信息才能渲染出一张图片。这使得检索三维模型的数据库是一件费事的工作。ShapeNet数据集\cite{Chang2015ShapeNetAI}将每一个类别的模型对齐到同一个朝向，并在固定的方向渲染了8张缩略图，ModelNet数据集\cite{Wu20143DSA}只提供了三维模型，这些方法并不能提供一个便捷的检索三维模型的方案。现有的处理三维模型的软件(如MeshLab \cite{Cignoni2008MeshLabAO})，提供用户一个拖拽视角的界面，让用户寻找最好的视角。如果能设计出生成最优视图的算法，将会便利检索三维模型数据库。

我们认为生成三维模型的最优视图至少包括两个部分，一个部分是选定最优的视角，另一部分是在这个选定的视角下渲染出带有材质的二位视图。第一个部分以往工作主要从图形学入手，通过在三维模型的顶点或是在二维视图上定义信息(熵)，取熵最大的视角作为最优视角。渲染材质的工作则集中利用了基于神经网络的生成模型，将材质生成问题定义为有条件的图片到图片翻译的问题。我们借鉴了这两方面方法的核心思想，并提出了新的生成三维模型最优视图的算法。

\section{相关工作}

\subsection{最优视角选择}

三维模型的最优视角选择任务旨在对给定的三维模型给出符合人类认知的最优的视角。这并不是一个良定义的问题，以往的研究方向往往采用在三维顶点或是二维像素上定义某种函数而将其转换为最优化问题。传统上认为最佳视角是包含最多信息的视角，不同的方法对信息的定义各不相同。在三维模型的二维视图上定义信息的文章主要包括：视角熵\cite{Vzquez2003AutomaticVS}，曲率熵\cite{Page2003ShapeAA}，轮廓熵\cite{Page2003ShapeAA}，在不同的视角的投影中取信息最大的投影作为最佳视角。\cite{Dutagaci2010ABF}文中对比了几种基于几何学的方法的结果和人为标注的最优视角的差别，文章得出 MeshSaliency\cite{Lee2005MeshS}和视角熵\cite{Vzquez2003AutomaticVS}的方法是效果最好的传统方法。Mesh Saliency\cite{Lee2005MeshS}通过在每一个三维定点定义与曲率有关的显著性，并将可见的显著性加和最大的视角定义为最佳视角。文章更加提出了一种在视角空间中类似梯度下降的方法寻找最优视角的方法，而不需在视角空间中方格搜索(grid search)最优视角。视角熵\cite{Vzquez2003AutomaticVS}的方法关注二维投影中可见的每一个三维面片的投影面积，并将投影面积构成的分布的熵最大的视角作为最优的视角。我们认为这些方法有时并不会产生令人满意的效果。他们破坏了同一类三维模型共享同一个最佳视角的规则，并且对三维模型的建模方式很敏感。本研究首先复现了经典的传统方法，在以后的行文中，采用Mesh Saliency和视角熵作为传统方法的代表，和我们提出的方法作比较。

\subsection{新视角生成}

新视角生成(novel view synthesis)旨在给定一个三维模型在一个或多个视角下的视图来生成新视角下的视图。因为不同视角下可见的像素不同，这个任务本质上是一个非良定义的问题，而需要足够强的先验知识和正则化约束来得到可接受的结果。以往解决新视角生成任务的方法大致可以分为两大类：基于几何学的和基于学习的方法。几何学的方法能够从输入图片显式的估计三维模型的结构和材质信息。Multi-view stereo\cite{Furukawa:2015:MST:2864699.2864700}方法可以通过多个视角的输入图片直接重构出三维模型。Flynn et al.\cite{7780964}提出的深度神经网络能够在不同视角的图片中进行插值。几何学的方法主要缺点是作为训练数据的三维模型难以获得，并且缺失的像素会导致错误的破洞填补(hole-filling)。基于学习的方法将新视角生成看作图片生成任务，或采取预测从原图片到目标图片的流\cite{Zhou2016ViewSB, sun2018multiview, olszewski2019tbn}的方式，或是采用某种正则化后直接生成每一个像素\cite{TDB16a, Huang_2017_ICCV, VIGAN, Park2017TransformationGroundedIG}。不同的方法针对它非良定义的特性使用了不同的正则化方法，如感知损失函数\cite{olszewski2019tbn}，生成对抗网络的损失函数\cite{Huang_2017_ICCV}和三维信息\cite{VIGAN}的方式。本文提出的模型可以看作\cite{VIGAN}的进一步拓展，将文中的三维信息进一步拆分为材质和内容信息，从而能够应用到材质迁移任务上。本文的方法在应用到新视角生成时，与流方法和像素生成的经典方法比较均有定性和定量结果的优势。

\subsection{材质迁移}

材质迁移旨在给定一张内容图片和一张材质图片(或材质属性)，生成一张具有前者内容和后者的材质信息的图片。一种通用的思路是从输入图片中编码出表征内容和表征材质的向量，如\cite{zheng2019joint, BeautyGlow, Li2018BeautyGANIF, Wu2019DisentanglingCA}。囿于成对训练数据缺失，材质迁移方法无法使用有监督的一范数或二范数损失函数，而需要采取独特的方法训练内容和材质的分离：如认为内容与材质信息分出VGG的不同层中\cite{7780634}，最小化循环损失(cycle loss)\cite{Lu2017ConditionalCF}或增加独特的判别器判定材质信息是否一致\cite{pix2pixSC2019, ma2017pose}。受AdaIN\cite{huang2017adain}启发，另一种融合材质和内容的方法是从内容或材质信息中回归参数来调整神经网络中间层激活量的大小和偏移，如\cite{zhu2019sean, park2019SPADE}。在三维领域，将一个三维模型的材质迁移至另一三维模型或直接生成三维模型的材质，可以通过直接在三维空间中生成每一个点或面片的颜色\cite{cmrKanazawa18}，也可以将三维模型投影至二维空间进行，如VON\cite{VON}。前者的方法生成结果较为模糊，并且因渲染过程不可导，需要采用近似渲染方法和可微分的渲染器\cite{Kato_2018_CVPR}。而后者能够借用图像领域材质迁移的成熟方法，取得更好的效果，并且作为这些方法中常用的深度图，也能够很好的在二维空间表征三维的信息。因此本文采用类似VON的模式，将三维模型渲染为深度图后，在深度图上渲染模型的材质信息。这个材质信息可以来自材质图片，也可以是真实图片，或者是随机采样的隐变量。定性和定量的结果表明我们的方法在生成图片质量上又一定的优势。

\section{研究内容}

本文研究生成三维模型的最优二维视图的任务。它要求对给定的三维模型我们能够生成符合人类观感的图片，这包括选取出一个合适的视角和生成具有真实性的材质两部分，其中材质可以来自参考图片也可以非条件的生成。我们回顾了在这个任务上前人的工作，综述并且实现了在视角选择和新视角生成领域经典的算法。在前人工作的基础上，我们提出了新的模型，将以往方法中的隐空间一分为二，分别编码三维模型的与视角无关的内容和材质信息，利用重构损失和对抗损失函数训练隐变量到图像空间的映射。我们认为最优视角是模型包含最多信息的视角，这种信息可以通过三维模型的某一视角下的图片重构不同视角下视图的精确度来衡量。在实验结果上，我们将我们的结果和复现的经典方法作比较，在定性结果和定量结果上，均体现出了优势。

\section{本文结构}

第一章引言介绍了本文的研究背景，在回顾了相关工作的基础上阐述了本文的研究内容。第二章将会介绍本文的实验原理，包括生成对抗网络(GAN)和变分自编码器(VAE)的推导，以及应用在本文的情境下的公式。其后，第三章将介绍本文提出的模型结构。我们的方法和其他方法的结果比较将放在第四章中。最后，我们在第五章总结本文的工作，并且展望未来可能的进展方向。



\clearpage

\chapter{实验原理}

本章会详细论证我们的方法的统计原理。在介绍了我们主要使用的变分自编码器(VAE)和生成对抗网络(GAN)之后，我们将推导出本文的方法的统计基础，为下一章介绍我们的方法的具体实现做好铺垫。

生成模型旨在学习一个分布$X \sim P(X)$，其中$X$可能是来自一个高维空间的数据点，例如我们实验的图像数据来自$128 \times 128 \times 3$维的分布。根据实际应用的需要，生成模型更关心的是从$P(X)$中采样，而不是数值的学习到对应于每一个$X$取值的$P(X)$，后者并不一定对前者有帮助。变分自编码器和生成对抗网络是成功的生成模型的两个例子，他们成功的利用了神经网络来拟合$P(X)$，并且对数据分布并没有很强的先验假设，因而成为广泛应用的生成模型。

\section{变分自编码器}

变分自编码器(VAE)主要可以用来无监督的学习复杂的数据分布。通过将直接采样$X$分解为先采样一个隐变量$z$，再通过学习到的映射将$z$映射到$X$的空间中，VAE将一个复杂的采样任务分解成一个从隐空间采样和一个学习映射的任务。这样做的优点有
\begin{itemize}
	\item 因为z从属的空间往往是简单的空间，例如标准正态分布，隐空间中的采样任务很容易实现
	\item 映射往往用神经网络来实现，这样容易通过随机梯度下降的方法优化，并且计算量也不会十分巨大
\end{itemize}

公式化的话，VAE期望优化训练集中的每一个数据点的概率值，$P(X) = \int P(X | z; \theta) P(z) dz$，其中由z生成X的分布是由$\theta$参数化的神经网络。为方便后续推导，我们假定 $P(X | z; \theta) = N(X | f(z; \theta), \sigma^2 \* I)$。如果直接对上式采样，将意味着很大的计算负担，并且对于大多数的$z$，$P(X|z)$都接近于$0$，很多样本是无意义的。因此，变分自编码器需要另一个函数$Q(z|X)$来拟合真实的$P(z|X)$分布，这也正是名字中变分一词的来历。其中$Q$分布通常也假定为正态分布，由另一个模型估计出来。这样，通过观察下式

\begin{equation}
	\begin{aligned}
		KL(Q(z|X) ||P(z|X)) &= E_{z \sim Q}[\log Q(z|X) - \log P(z | X)] \\
		&= E_{z \sim Q}[\log Q(z|X) - \log P(X | z) - \log P(z)] + \log P(X) \\
	\end{aligned}
\end{equation}

我们移项得到

\begin{equation}
	\begin{aligned}
		\log P(X) &= E_{z \sim Q}[\log P(X|z)] - KL(Q(z|X) | P(z)) + KL(Q(z|X) || P(z|X)) \\
		&\geq E_{z \sim Q}[\log P(X|z)] - KL(Q(z|X) | P(z))
	\end{aligned}
\end{equation}

上式左边正是我们想要优化的：最大化训练集的概率。而上式右边两项又可以通过神经网络求出：两项分别代表重构图片的概率，在高斯假定的情况下近似为L2误差，和两个正态分布之间的KL散度。我们通过优化右边这个变分下界，来近似的最大化训练集的概率。

从表示学习(representation learning)的角度看，VAE学习到了高维空间图像的一个低维表示。在表示学习中，一个重要的任务是学习到一个分离(disentangle)的表示，这种表示既存储了高维空间向量的全部信息，本身又有很强的可解释性。例如对于手写字符图像任务，一种可行的分离表示可以是$n$维隐变量的第一个维度表示字符的类别，第二/三个表示字符的斜度/粗细，每一个维度表征数据集中变化的一个因子（factor of variation）。这种表示意味着我们可以通过调整一个维度来调整生成图像的某一个特征，对风格迁移任务有很大的帮助。在传统的VAE基础之上，研究者提出了不同的VAE变种来提升VAE编码的分离能力，如\cite{Burgess2018UnderstandingDI, RubSchTol18b}。他们可以看作在传统的VAE基础上加入$\int Q(z|X) P(X) dX$，用不同的方式实现这个后验分布，优化模型的分离能力。

上述的分离学习方法专注于在隐变量的每一个维度都分离的控制不同的变化因子。在细粒度的分离学习之外，我们也可以将VAE的隐变量分成几组，每组控制到不同的特征。\cite{vunet2018}文认为图像是由形状(shape)和外表(appearance)共同控制生成，文中提出了一个条件的U-Net，用形状和RGB图片生成新的图片。\cite{Wu2019DisentanglingCA}文用无监督的方法分离图像中的结构和风格信息。本文同样以VAE为理论基础，专注于分离图片中几何信息和风格信息。我们在理论推导方面受到了上两文的启发，但本文选取了深度图表示结构信息，而不是隐变量\cite{vunet2018}或关键点\cite{Wu2019DisentanglingCA}，另外在模型实现方面也和上两文有所不同。此外，应对材质迁移后的图片训练集中无监督的情况，我们引入了生成对抗网络来优化生成结果，这些在本章的后续章节中都会详细论述。

\section{生成对抗网络}

生成对抗网络（GAN）是另一种常用的生成模型，与VAE不同，它并不直接估计数据的概率分布，而是间接的近似$P(X)$。GAN的模型分成两个部分：生成器和判别器，前者能采样出和训练集数据同分布的样本，而判别器能够判别一个给定样本是否来自于训练集的分布，二者通常是以神经网络参数化的模型。GAN的训练通过上述两方的博弈，最终得到一个纳什均衡。和VAE相比，GAN的优化目标并没有用到变分下界，并且神经网络的优秀拟合能力使得GAN理论上是能够拟合任何分布的。另外，由于GAN的优化目标是判断训练集的分布和生成器生成的分布是否拟合，对每一个样本从整体上判断生成质量，经验上它所生成的样本往往更具真实性。而VAE由于损失函数中的重构损失往往由L1/2损失函数估计，这种像素级别的损失函数偏向于生成一个平均图像，导致生成图像往往边界不清晰，图像模糊。

公式化论述的话，我们遵从GAN原文的记法，训练集$x \sim p_{data}(x)$，生成器和判别器分别记为$G, D$，是两个神经网络，前者接受一个随机变量$z$作为输入，它通常来自一个标准正态分布，力图输出符合分布$p_{data}$的样本，后者类似一个分类器，接受一个样本，输出$[0, 1]$之间的数值，力图对训练集中样本输出1，而对生成器生成的样本输出0。按前文的说明，GAN要优化的是下式

\begin{equation}
	\min_{G} \max_{D} V(D, G) = E_{x \sim p_{data}(x)}[\log D(x)] + E_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]
\end{equation}

GAN原文中证明了上式事实上等价于优化生成器的分布和训练集的Jensen-Shannon散度，亦即对称的KL散度，并且也在理论上论述了最优解的存在性。


\clearpage







\addcontentsline{toc}{chapter}{参考文献}
\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{参考文献}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{参考文献}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}






\bibliographystyle{unsrt}
\bibliography{ref}
\clearpage





\linespread{1}\selectfont
\normalsize
%小四号，中文宋体，英文Time new roman，1倍行距
\chapter*{本科期间的主要工作和成果}

\noindent 本科期间参加的主要科研项目

\noindent 本研基金
\begin{enumerate}
	\item 国家创新训练计划. 基金类型. 连宙辉. 2018-2019
\end{enumerate}



\addcontentsline{toc}{chapter}{本科期间的主要工作和成果}
\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{本科期间的主要工作和成果}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{本科期间的主要工作和成果}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}
\clearpage





\linespread{1.5}\selectfont
\normalsize
\chapter*{致谢}

感谢

\addcontentsline{toc}{chapter}{致谢}
\fancypagestyle{plain}
{
	\fancyhf{}
	\fancyhead[RE,RO]{致谢}
	\fancyhead[LE,LO]{北京大学本科生毕业论文}
	\fancyfoot[CO,CE]{~\thepage~}
	\renewcommand{\headrulewidth}{0.7pt}
	\renewcommand{\footrulewidth}{0pt}
}
\fancyhf{}
\fancyhead[RE,RO]{致谢}
\fancyhead[LE,LO]{北京大学本科生毕业论文}
\fancyfoot[CO,CE]{~\thepage~}
\renewcommand{\headrulewidth}{0.7pt}
\renewcommand{\footrulewidth}{0pt}





\end{document}
