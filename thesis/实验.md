### 实验结果

本章将集中展示我们的方法的实验结果，包括进行实验的数据集，生成图片的定性定量结果和与其他方法的比较。

##### 数据集

为了公平比较，我们使用了与af相同的数据集进行实验。af文将shapenet-car分支下的模型分成两部分，分别作为训练集和测试集，训练集有xxxx个模型，测试集有yyyy个模型。对每一个模型，af在54个视角下渲染，分别是方位角0递增到340，步长为20，仰角分别取0，10，20。我们使用blender的python脚本先渲染出这些视角下的深度图和RGB图片，以减少训练时渲染的时间开销。af和3dconv开源的代码中采用了自己的渲染器，为保证实验结果的公平性，这些方法都是在我们渲染出的数据集上实验。对于真实数据，我们采用

**3D Object Representations for Fine-Grained Categorization**

中提出的汽车数据集，其中分别包含8144和8401张训练和测试的汽车图片。因为我们的模型并没有编码背景的能力，我们用预训练的场景分割算法分割出包含汽车的部分。为保证真实图片的数据分布和我们的合成图片尽可能的贴合，我们删去了物体大小过小，和物体不居中的图像。

##### 新视角生成

为衡量我们方法中分离内容与材质特征的部分的有效性，下面展示我们的方法和其他没有分离特征的新视角生成的方法比较的结果。我们用L1误差和FID来比较各个方法的结果，其中前者用每个像素位置的平均误差度量生成图片的准确性，后者用生成图片和真实图片这两个分布的距离度量生成图片的真实性。我们用新视角生成中两个经典的算法af和3dconv作为比较标准，前者作为流模型的代表，后者做为作为直接生成像素方法的代表。遵循af文中惯例，我们从测试集中随机选取出10k张图片计算L1损失和FID。观察到，我们的方法在生成图片准确性和真实性上都和新视角生成方法具有可比性。这说明我们的分离内容与材质的生成模式是有效的，隐变量的分离并不以生成图片的质量为代价，反而增强了生成图片的质量。我们在表中同样列出了我们使用的各个训练技巧的之后的结果。【】图中展示了我们的算法在连续变换视角的条件下的生成结果，模型有能力提取到与视角无关的内容信息，并用于生成各个视角下的视图。



##### 材质迁移

分离隐变量中的材质部分和内容部分也使得我们的模型能够实现材质迁移的效果：即生成融合两张图片的材质和内容特征的视图，这是新视角生成方法所不能做到的。我们在下面展示材质迁移的结果。可以看到我们的算法能够迁移基本的色彩信息，同时完美的保留内容图片的内容结构。观察到申城结果在细节方面有一些不足之处，在后文中会详细讨论其原因。【】图中展示了插值实验的结果，我们对最左端和最右端的图片提取到的材质变量进行插值，生成中间的各个图像。实验结果表明了模型能够学习到连续的材质隐变量流形，也即我们对材质变量的高斯分布假设是有效的。



##### 最优视角

我们认为最优视角是在考虑了材质的条件下包含最多信息的视角，具体的计算方法在理论一节中有详细叙述。和以往几何学的方法不同，我们的方法直接在RGB图片的尺度上选择最优视角。这种方法即考虑到材质信息，也没有忽略掉模型的几何结构的影响：我们观察到几何结构复杂的地方通常也是材质信息高频的区域。定性试验上，我们和【】文中列出的方法视角熵和mesh saliency进行了比较。为保证公平性，两种方法也是从af文定义的54个视角中选取最优视角。我们参考原文的描述和开源的方法复现了这两种方法，在测试集上的定性结果在图【】中。Mesh Saliency因为倾向于选择能看到模型几何上更加复杂的区域，它在汽车数据集上倾向于选择侧面的视角，没有很好的把握住模型上材质变化的区域。



作为补充，为说明我们这种定义最优视角的方式是有意义的，我们展示了在不同的输入和输出视角下重构图片的误差大小。下图中横轴代表输入视角、纵轴代表输出视角。我们观察到输出视角对重构图片的误差影响很小，而输入视角几乎决定了生成图片的误差大小。我们认为这是因为我们提取模型视角无关的内容和材质信息的结构产生了这个结果。对于包含更多细节的视角，例如包含车头或车尾的视角，模型能够提取到更多更完整的内容和材质信息，因此在多视角下都能够重构出更好的视图，反之则在各个视角下重构结果都变差。因此可以通过重构误差来反推出视角包含信息的多少。相反，基于流的生成方法，生成图片的误差取决于输入输出视角之间的差距，不能够用生成图片的期望误差来评判输入视角的好坏。